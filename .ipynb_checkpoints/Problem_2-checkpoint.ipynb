{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Getting familiar with TensorFlow\n",
    "\n",
    "*TensorFlow* is one of the most popular deep learning framework developed by Google. If you are new to TensorFlow, please read and play with the sample in [Getting started with TensorFlow](https://www.tensorflow.org/get_started/get_started) to get started.\n",
    "\n",
    "* <b>Learning Objective:</b> In Problem 1, you implemented a fully connected network from scratch on your own. Very tedious to do it all by yourself, right? Well, we actually feel the same thing, that's why we are using tools instead of doing everything from scratch. For this part of the assignment, we will familiarize you with a widely-used deep learning framework developed by Google, TensorFlow and walk you through convolutional neural networks and show how to train them.\n",
    "* <b>Provided Codes:</b> We provide the Template class for a simple CNN model as BaseModel, predefined skeletons for conv2d() and max_pool(), as well as the dataset preprocessing parts.\n",
    "* <b>TODOs:</b> You are asked to implement the BaseModel following the detailed instructions and design your own model in YourModel to achieve a reasonably good performance for classification task on CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/KWAI/anaconda2/envs/csci566/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/KWAI/anaconda2/envs/csci566/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/KWAI/anaconda2/envs/csci566/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/KWAI/anaconda2/envs/csci566/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/KWAI/anaconda2/envs/csci566/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/KWAI/anaconda2/envs/csci566/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/KWAI/anaconda2/envs/csci566/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/KWAI/anaconda2/envs/csci566/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/KWAI/anaconda2/envs/csci566/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/KWAI/anaconda2/envs/csci566/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/KWAI/anaconda2/envs/csci566/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/KWAI/anaconda2/envs/csci566/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version 1.14.0\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "# Add whatever you want\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from lib.datasets import CIFAR10_tf\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# We recommend to use tensorflow==1.14.0\n",
    "print(\"TensorFlow Version {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets\n",
    "Download [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz) and load the dataset. In this assignment, we will use the standard 50,000 images for training and 10,000 images for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "num_training = 49000\n",
    "num_validation = 50000 - num_training\n",
    "num_test = 10000\n",
    "\n",
    "data = CIFAR10_tf(num_training=num_training,\n",
    "                  num_validation=num_validation,\n",
    "                  num_test=num_test)\n",
    "\n",
    "# Load cifar-10 data\n",
    "X_train, Y_train = data['data_train'], data['labels_train']\n",
    "X_val, Y_val = data['data_val'], data['labels_val']\n",
    "X_test, Y_test = data['data_test'], data['labels_test']\n",
    "\n",
    "# Check the shape of the dataset\n",
    "assert X_train.shape == (num_training, 32, 32, 3)\n",
    "assert Y_train.shape == (num_training, )\n",
    "assert X_val.shape == (num_validation, 32, 32, 3)\n",
    "assert Y_val.shape == (num_validation, )\n",
    "assert X_test.shape == (num_test, 32, 32, 3)\n",
    "assert Y_test.shape == (10000, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2-1 [10pt]\n",
    "\n",
    "Using the code provided, implement a neural network architecture with an optimization routine according to the specification provided below.\n",
    "\n",
    "**Model:**\n",
    "- Input image with the size 32x32x3\n",
    "- 7x7 convolutional layer with 32 filters, stride of 1, and padding 'SAME'\n",
    "- ReLU activation layer\n",
    "- 3x3 max pooling layer with a stride of 2\n",
    "- 5x5 convolutional layer with 64 filters, stride of 1, and padding 'SAME'\n",
    "- ReLU activation layer\n",
    "- 3x3 max pooling layer with a stride of 2\n",
    "- Flatten layer (8x8x64 -> 4096)\n",
    "- Fully-connected layer with 384 output units (4096 -> 384)\n",
    "- ReLU activation layer\n",
    "- Fully-connected layer with 10 output units (384 -> 10)\n",
    "- Output logits (10)\n",
    "\n",
    "**Optimizer:**\n",
    "- Adam optimizer\n",
    "\n",
    "**Learning rate:**\n",
    "- Set start learning rate as 5e-4 and apply exponential decay every 500 steps with a base of 0.96\n",
    "- Use 'tf.train.exponential_decay' and 'tf.train.AdamOptimizer'\n",
    "\n",
    "**Loss:**\n",
    "- Softmax cross entropy loss\n",
    "- Use 'tf.nn.softmax_cross_entropy_with_logits_v2'\n",
    "\n",
    "\n",
    "\n",
    "Your model **should** achieve about 55% accuracy on test set in 5 epochs using provided evaluation code.\n",
    "\n",
    "You can modify the template code as you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define your layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define max pooling and conv layers\n",
    "\n",
    "def conv2d(input, kernel_size, stride, num_filter):\n",
    "    stride_shape = [1, stride, stride, 1]\n",
    "    filter_shape = [kernel_size, kernel_size, input.get_shape()[3], num_filter]\n",
    "\n",
    "    W = tf.get_variable('w', filter_shape, tf.float32, tf.random_normal_initializer(0.0, 0.02))\n",
    "    b = tf.get_variable('b', [1, 1, 1, num_filter], initializer=tf.constant_initializer(0.0))\n",
    "    return tf.nn.conv2d(input, W, stride_shape, padding='SAME') + b\n",
    "\n",
    "def max_pool(input, kernel_size, stride):\n",
    "    ksize = [1, kernel_size, kernel_size, 1]\n",
    "    strides = [1, stride, stride, 1]\n",
    "    return tf.nn.max_pool(input, ksize=ksize, strides=strides, padding='SAME')\n",
    "\n",
    "#############################################################################\n",
    "# TODO: Complete the following functions                                    #\n",
    "#############################################################################\n",
    "def flatten(input):\n",
    "    \"\"\"\n",
    "        - input: input tensors\n",
    "    \"\"\"\n",
    "    return tf.layers.flatten(input)\n",
    "\n",
    "def fc(input, num_output):\n",
    "    \"\"\"\n",
    "        - input: input tensors\n",
    "        - num_output: int, the output dimension\n",
    "    \"\"\"\n",
    "    return tf.layers.dense(input, num_output)\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(object):\n",
    "    def __init__(self):\n",
    "        self.num_epoch = 5\n",
    "        self.batch_size = 64\n",
    "        self.log_step = 50\n",
    "        self._build_model()\n",
    "\n",
    "    def _model(self):\n",
    "        print('-' * 5 + '  Sample model  ' + '-' * 5)\n",
    "\n",
    "        print('intput layer: ' + str(self.X.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('conv1'):\n",
    "            self.conv1 = conv2d(self.X, 7, 1, 32)\n",
    "            self.relu1 = tf.nn.relu(self.conv1)\n",
    "            self.pool1 = max_pool(self.relu1, 3, 2)            \n",
    "            print('conv1 layer: ' + str(self.pool1.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('conv2'):\n",
    "            #############################################################################\n",
    "            # TODO: Complete the following functions                                    #\n",
    "            #############################################################################\n",
    "            self.conv2 = conv2d(self.pool1, 5, 1, 64)\n",
    "            self.relu2 = tf.nn.relu(self.conv2)\n",
    "            self.pool2 = max_pool(self.relu2, 3, 2)            \n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            print('conv2 layer: ' + str(self.pool2.get_shape()))\n",
    "\n",
    "        #############################################################################\n",
    "        # TODO: Flatten the output tensor from conv2 layer                          #\n",
    "        #############################################################################\n",
    "        self.flat = flatten(self.pool2)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################      \n",
    "        print('flat layer: ' + str(self.flat.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('fc3'):\n",
    "            #############################################################################\n",
    "            # TODO: Complete the following functions                                    #\n",
    "            #############################################################################\n",
    "            self.fc3 = fc(self.flat, 384)\n",
    "            self.relu3 = tf.nn.relu(self.fc3)\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            print('fc3 layer: ' + str(self.relu3.get_shape()))\n",
    "            \n",
    "        with tf.variable_scope('fc4'):\n",
    "            #############################################################################\n",
    "            # TODO: Complete the following functions                                    #\n",
    "            #############################################################################\n",
    "            self.fc4 = fc(self.relu3, 10)\n",
    "            #self.relu4 = tf.nn.relu(self.fc4)\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            print('fc4 layer: ' + str(self.fc4.get_shape()))\n",
    "            \n",
    "        # Return the last layer\n",
    "        return self.fc4\n",
    "\n",
    "    def _input_ops(self):\n",
    "        # Placeholders\n",
    "        self.X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        self.Y = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "        #############################################################################\n",
    "        # TODO: You can add any placeholders                                        #\n",
    "        #############################################################################\n",
    "\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def _build_optimizer(self):\n",
    "        #############################################################################\n",
    "        # TODO: Adam optimizer 'self.train_op' that minimizes 'self.loss_op'        #\n",
    "        #############################################################################\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        learning_rate = tf.train.exponential_decay(5e-4, global_step, 500, 0.96)\n",
    "        self.train_op = tf.train.AdamOptimizer(learning_rate).minimize(self.loss_op, global_step=global_step)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "        \n",
    "    def _loss(self, labels, logits):\n",
    "        #############################################################################\n",
    "        # TODO: Softmax cross entropy loss 'self.loss_op'                           #\n",
    "        #############################################################################\n",
    "        self.loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels, logits))\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Define input variables\n",
    "        self._input_ops()\n",
    "\n",
    "        # Convert Y to one-hot vector\n",
    "        labels = tf.one_hot(self.Y, 10)\n",
    "\n",
    "        # Build a model and get logits\n",
    "        logits = self._model()\n",
    "\n",
    "        # Compute loss\n",
    "        self._loss(labels, logits)\n",
    "        \n",
    "        # Build optimizer\n",
    "        self._build_optimizer()\n",
    "\n",
    "        # Compute accuracy\n",
    "        predict = tf.argmax(logits, 1)\n",
    "        correct = tf.equal(predict, self.Y)\n",
    "        self.accuracy_op = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        \n",
    "    def train(self, sess, X_train, Y_train, X_val, Y_val):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        step = 0\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        print('-' * 5 + '  Start training  ' + '-' * 5)\n",
    "        for epoch in range(self.num_epoch):\n",
    "            print('train for epoch %d' % epoch)\n",
    "            for i in range(num_training // self.batch_size):\n",
    "                X_ = X_train[i * self.batch_size:(i + 1) * self.batch_size][:]\n",
    "                Y_ = Y_train[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "\n",
    "                #############################################################################\n",
    "                # TODO: You can change feed data as you want                                #\n",
    "                #############################################################################\n",
    "                feed_dict = {self.X:X_, self.Y:Y_}\n",
    "                #############################################################################\n",
    "                #                             END OF YOUR CODE                              #\n",
    "                #############################################################################\n",
    "                fetches = [self.train_op, self.loss_op, self.accuracy_op]\n",
    "\n",
    "                _, loss, accuracy = sess.run(fetches, feed_dict=feed_dict)\n",
    "                losses.append(loss)\n",
    "                accuracies.append(accuracy)\n",
    "\n",
    "                if step % self.log_step == 0:\n",
    "                    print('iteration (%d): loss = %.3f, accuracy = %.3f' %\n",
    "                        (step, loss, accuracy))\n",
    "                step += 1\n",
    "\n",
    "            # Print validation results\n",
    "            print('validation for epoch %d' % epoch)\n",
    "            val_accuracy = self.evaluate(sess, X_val, Y_val)\n",
    "            print('-  epoch %d: validation accuracy = %.3f' % (epoch, val_accuracy))\n",
    "            \n",
    "        #############################################################################\n",
    "        # TODO: Plot training curve                                                 #\n",
    "        #############################################################################\n",
    "        # Graph 1. X: iteration (training step), Y: training loss\n",
    "\n",
    "        # Graph 2. X: iteration (training step), Y: training accuracy\n",
    "        \n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def evaluate(self, sess, X_eval, Y_eval):\n",
    "        eval_accuracy = 0.0\n",
    "        eval_iter = 0\n",
    "        for i in range(X_eval.shape[0] // self.batch_size):\n",
    "            X_ = X_eval[i * self.batch_size:(i + 1) * self.batch_size][:]\n",
    "            Y_ = Y_eval[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            \n",
    "            #############################################################################\n",
    "            # TODO: You can change feed data as you want                                #\n",
    "            #############################################################################\n",
    "            feed_dict = {self.X:X_, self.Y:Y_}\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            accuracy = sess.run(self.accuracy_op, feed_dict=feed_dict)\n",
    "            eval_accuracy += accuracy\n",
    "            eval_iter += 1\n",
    "        return eval_accuracy / eval_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  Sample model  -----\n",
      "intput layer: (?, 32, 32, 3)\n",
      "conv1 layer: (?, 16, 16, 32)\n",
      "conv2 layer: (?, 8, 8, 64)\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x134644dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x134644dd8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x134644dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x134644dd8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "flat layer: (?, 4096)\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x137a2ddd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x137a2ddd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x137a2ddd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x137a2ddd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "fc3 layer: (?, 384)\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1372a5278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1372a5278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1372a5278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1372a5278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "fc4 layer: (?, 10)\n",
      "-----  Start training  -----\n",
      "train for epoch 0\n",
      "iteration (0): loss = 14.954, accuracy = 0.094\n",
      "iteration (50): loss = 2.222, accuracy = 0.141\n",
      "iteration (100): loss = 2.211, accuracy = 0.172\n",
      "iteration (150): loss = 2.076, accuracy = 0.234\n",
      "iteration (200): loss = 1.696, accuracy = 0.391\n",
      "iteration (250): loss = 1.878, accuracy = 0.297\n",
      "iteration (300): loss = 1.799, accuracy = 0.234\n",
      "iteration (350): loss = 1.574, accuracy = 0.391\n",
      "iteration (400): loss = 1.501, accuracy = 0.406\n",
      "iteration (450): loss = 1.499, accuracy = 0.516\n",
      "iteration (500): loss = 1.752, accuracy = 0.406\n",
      "iteration (550): loss = 1.453, accuracy = 0.500\n",
      "iteration (600): loss = 1.760, accuracy = 0.422\n",
      "iteration (650): loss = 1.693, accuracy = 0.438\n",
      "iteration (700): loss = 1.428, accuracy = 0.500\n",
      "iteration (750): loss = 1.935, accuracy = 0.359\n",
      "validation for epoch 0\n",
      "-  epoch 0: validation accuracy = 0.491\n",
      "train for epoch 1\n",
      "iteration (800): loss = 1.348, accuracy = 0.469\n",
      "iteration (850): loss = 1.455, accuracy = 0.531\n",
      "iteration (900): loss = 1.240, accuracy = 0.562\n",
      "iteration (950): loss = 1.495, accuracy = 0.469\n",
      "iteration (1000): loss = 1.475, accuracy = 0.500\n",
      "iteration (1050): loss = 1.369, accuracy = 0.484\n",
      "iteration (1100): loss = 1.431, accuracy = 0.422\n",
      "iteration (1150): loss = 1.292, accuracy = 0.453\n",
      "iteration (1200): loss = 1.501, accuracy = 0.469\n",
      "iteration (1250): loss = 1.080, accuracy = 0.578\n",
      "iteration (1300): loss = 1.108, accuracy = 0.594\n",
      "iteration (1350): loss = 1.400, accuracy = 0.500\n",
      "iteration (1400): loss = 1.091, accuracy = 0.594\n",
      "iteration (1450): loss = 1.481, accuracy = 0.469\n",
      "iteration (1500): loss = 1.390, accuracy = 0.516\n",
      "validation for epoch 1\n",
      "-  epoch 1: validation accuracy = 0.561\n",
      "train for epoch 2\n",
      "iteration (1550): loss = 1.222, accuracy = 0.641\n",
      "iteration (1600): loss = 1.433, accuracy = 0.422\n",
      "iteration (1650): loss = 1.280, accuracy = 0.594\n",
      "iteration (1700): loss = 1.423, accuracy = 0.453\n",
      "iteration (1750): loss = 1.156, accuracy = 0.578\n",
      "iteration (1800): loss = 1.322, accuracy = 0.578\n",
      "iteration (1850): loss = 1.147, accuracy = 0.594\n",
      "iteration (1900): loss = 0.987, accuracy = 0.625\n",
      "iteration (1950): loss = 1.368, accuracy = 0.531\n",
      "iteration (2000): loss = 1.150, accuracy = 0.656\n",
      "iteration (2050): loss = 1.161, accuracy = 0.562\n",
      "iteration (2100): loss = 1.232, accuracy = 0.656\n",
      "iteration (2150): loss = 1.297, accuracy = 0.625\n",
      "iteration (2200): loss = 1.285, accuracy = 0.531\n",
      "iteration (2250): loss = 0.981, accuracy = 0.672\n",
      "validation for epoch 2\n",
      "-  epoch 2: validation accuracy = 0.598\n",
      "train for epoch 3\n",
      "iteration (2300): loss = 1.239, accuracy = 0.578\n",
      "iteration (2350): loss = 1.062, accuracy = 0.578\n",
      "iteration (2400): loss = 0.917, accuracy = 0.672\n",
      "iteration (2450): loss = 0.969, accuracy = 0.656\n",
      "iteration (2500): loss = 1.007, accuracy = 0.625\n",
      "iteration (2550): loss = 1.009, accuracy = 0.656\n",
      "iteration (2600): loss = 1.329, accuracy = 0.578\n",
      "iteration (2650): loss = 1.110, accuracy = 0.547\n",
      "iteration (2700): loss = 1.111, accuracy = 0.625\n",
      "iteration (2750): loss = 1.108, accuracy = 0.641\n",
      "iteration (2800): loss = 1.004, accuracy = 0.578\n",
      "iteration (2850): loss = 1.087, accuracy = 0.641\n",
      "iteration (2900): loss = 0.885, accuracy = 0.703\n",
      "iteration (2950): loss = 1.241, accuracy = 0.594\n",
      "iteration (3000): loss = 1.024, accuracy = 0.625\n",
      "iteration (3050): loss = 1.061, accuracy = 0.656\n",
      "validation for epoch 3\n",
      "-  epoch 3: validation accuracy = 0.596\n",
      "train for epoch 4\n",
      "iteration (3100): loss = 0.874, accuracy = 0.703\n",
      "iteration (3150): loss = 1.054, accuracy = 0.625\n",
      "iteration (3200): loss = 1.060, accuracy = 0.656\n",
      "iteration (3250): loss = 0.882, accuracy = 0.703\n",
      "iteration (3300): loss = 0.891, accuracy = 0.703\n",
      "iteration (3350): loss = 0.887, accuracy = 0.672\n",
      "iteration (3400): loss = 1.094, accuracy = 0.641\n",
      "iteration (3450): loss = 0.826, accuracy = 0.703\n",
      "iteration (3500): loss = 0.940, accuracy = 0.750\n",
      "iteration (3550): loss = 1.126, accuracy = 0.688\n",
      "iteration (3600): loss = 0.788, accuracy = 0.719\n",
      "iteration (3650): loss = 0.838, accuracy = 0.641\n",
      "iteration (3700): loss = 0.823, accuracy = 0.703\n",
      "iteration (3750): loss = 0.694, accuracy = 0.750\n",
      "iteration (3800): loss = 0.970, accuracy = 0.625\n",
      "validation for epoch 4\n",
      "-  epoch 4: validation accuracy = 0.610\n",
      "***** test accuracy: 0.591\n",
      "Model saved in lib/tf_models/problem2/csci-599_sample.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Clear old computation graphs\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Train our sample model\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "    with tf.device('/cpu:0'):\n",
    "        model = BaseModel()\n",
    "        model.train(sess, X_train, Y_train, X_val, Y_val)\n",
    "        accuracy = model.evaluate(sess, X_test, Y_test)\n",
    "        print('***** test accuracy: %.3f' % accuracy)\n",
    "        saver = tf.train.Saver()\n",
    "        model_path = saver.save(sess, \"lib/tf_models/problem2/csci-599_sample.ckpt\")\n",
    "        print(\"Model saved in %s\" % model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2-2 [15pt]\n",
    "\n",
    "Implement your own model. \n",
    "\n",
    "You can modify the template code as you want and you can use GPU for fast training. For GPU usage, simply change the following line of the training block:  \n",
    "from `with tf.device('/cpu:0')` to `with tf.device('/GPU:0')` and you can set your desired device number.\n",
    "\n",
    "These are the techniques that you can try:\n",
    "- Data preprocessing\n",
    "- Data augmentation\n",
    "- Batch normalization\n",
    "- Dropout\n",
    "- More convolutional layers\n",
    "- More training epochs\n",
    "- Learning rate decay\n",
    "- Any other models and techniqes\n",
    "\n",
    "The rubrics for this question is:\n",
    "* 15 points when test accuracy >= 75%\n",
    "* 10 points when test accuracy >= 70%\n",
    "* 5 points when test accuracy >= 60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourModel(BaseModel):\n",
    "    def __init__(self):\n",
    "        super(YourModel, self).__init__()\n",
    "        self.num_epoch = 30\n",
    "        \n",
    "    def _model(self):\n",
    "        print('-' * 5 + '  Sample model  ' + '-' * 5)\n",
    "\n",
    "        with tf.variable_scope('conv1'):\n",
    "            self.conv1 = conv2d(self.X, 3, 2, 48)\n",
    "            self.relu1 = tf.nn.relu(self.conv1)\n",
    "            self.pool1 = max_pool(self.relu1, 2, 2)\n",
    "            self.bn1 = tf.layers.batch_normalization(self.pool1)\n",
    "            print('conv1 layer: ' + str(self.bn1.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('conv2'):\n",
    "            self.conv2 = conv2d(self.bn1, 3, 1, 96)\n",
    "            self.relu2 = tf.nn.relu(self.conv2)\n",
    "            self.pool2 = max_pool(self.relu2, 3, 2)\n",
    "            self.bn2 = tf.layers.batch_normalization(self.pool2)\n",
    "            print('conv2 layer: ' + str(self.bn2.get_shape()))\n",
    "            \n",
    "        with tf.variable_scope('conv3'):\n",
    "            self.conv3 = conv2d(self.bn2, 3, 1, 192)\n",
    "            self.relu3 = tf.nn.relu(self.conv3)\n",
    "            \n",
    "        with tf.variable_scope('conv4'):\n",
    "            self.conv4 = conv2d(self.relu3, 3, 1, 192)\n",
    "            self.relu4 = tf.nn.relu(self.conv4)\n",
    "            \n",
    "        with tf.variable_scope('conv5'):\n",
    "            self.conv5 = conv2d(self.relu4, 3, 1, 256)\n",
    "            self.relu5 = tf.nn.relu(self.conv5)\n",
    "            self.pool5 = max_pool(self.relu5, 3, 2)\n",
    "            self.bn5 = tf.layers.batch_normalization(self.pool5)\n",
    "            print('deep conv layer: ' + str(self.bn5.get_shape()))\n",
    "\n",
    "\n",
    "        self.flat = flatten(self.bn5)     \n",
    "        print('flat layer: ' + str(self.flat.get_shape()))\n",
    "\n",
    "        with tf.variable_scope('fc6'):\n",
    "            self.fc6 = fc(self.flat, 512)\n",
    "            self.tanh6 = tf.math.tanh(self.fc6)\n",
    "            self.dropout6 = tf.nn.dropout(self.tanh6, 0.5)\n",
    "            print('fc3 layer: ' + str(self.dropout6.get_shape()))\n",
    "            \n",
    "        with tf.variable_scope('fc7'):\n",
    "            self.fc7 = fc(self.dropout6, 256)\n",
    "            self.tanh7 = tf.math.tanh(self.fc7)\n",
    "            self.dropout7 = tf.nn.dropout(self.tanh7, 0.5)\n",
    "            print('fc4 layer: ' + str(self.dropout7.get_shape()))\n",
    "            \n",
    "        with tf.variable_scope('fc8'):\n",
    "            self.fc8 = fc(self.dropout7, 10)\n",
    "            #self.tanh8 = tf.math.tanh(self.fc8)\n",
    "            print('fc5 layer: ' + str(self.fc8.get_shape()))\n",
    "            \n",
    "        # Return the last layer\n",
    "        return self.fc8\n",
    "\n",
    "    def _input_ops(self):\n",
    "        # Placeholders\n",
    "        self.X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "        self.Y = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "        #############################################################################\n",
    "        # TODO: You can add any placeholders                                        #\n",
    "        #############################################################################\n",
    "\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def _build_optimizer(self):\n",
    "        #############################################################################\n",
    "        # TODO: Adam optimizer 'self.train_op' that minimizes 'self.loss_op'        #\n",
    "        #############################################################################\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        learning_rate = tf.train.exponential_decay(5e-4, global_step, 500, 0.96)\n",
    "        self.train_op = tf.train.AdamOptimizer(learning_rate).minimize(self.loss_op, global_step=global_step)\n",
    "        #self.train_op = tf.train.AdamOptimizer().minimize(self.loss_op)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "        \n",
    "    def _loss(self, labels, logits):\n",
    "        #############################################################################\n",
    "        # TODO: Softmax cross entropy loss 'self.loss_op'                           #\n",
    "        #############################################################################\n",
    "        self.loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels, logits))\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Define input variables\n",
    "        self._input_ops()\n",
    "\n",
    "        # Convert Y to one-hot vector\n",
    "        labels = tf.one_hot(self.Y, 10)\n",
    "\n",
    "        # Build a model and get logits\n",
    "        logits = self._model()\n",
    "\n",
    "        # Compute loss\n",
    "        self._loss(labels, logits)\n",
    "        \n",
    "        # Build optimizer\n",
    "        self._build_optimizer()\n",
    "\n",
    "        # Compute accuracy\n",
    "        predict = tf.argmax(logits, 1)\n",
    "        correct = tf.equal(predict, self.Y)\n",
    "        self.accuracy_op = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        \n",
    "    def train(self, sess, X_train, Y_train, X_val, Y_val):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        step = 0\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        print('-' * 5 + '  Start training  ' + '-' * 5)\n",
    "        for epoch in range(self.num_epoch):\n",
    "            print('train for epoch %d' % epoch)\n",
    "            for i in range(num_training // self.batch_size):\n",
    "                X_ = X_train[i * self.batch_size:(i + 1) * self.batch_size][:]\n",
    "                Y_ = Y_train[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "\n",
    "                #############################################################################\n",
    "                # TODO: You can change feed data as you want                                #\n",
    "                #############################################################################\n",
    "                feed_dict = {self.X:X_, self.Y:Y_}\n",
    "                #############################################################################\n",
    "                #                             END OF YOUR CODE                              #\n",
    "                #############################################################################\n",
    "                fetches = [self.train_op, self.loss_op, self.accuracy_op]\n",
    "\n",
    "                _, loss, accuracy = sess.run(fetches, feed_dict=feed_dict)\n",
    "                losses.append(loss)\n",
    "                accuracies.append(accuracy)\n",
    "\n",
    "                if step % self.log_step == 0:\n",
    "                    print('iteration (%d): loss = %.3f, accuracy = %.3f' %\n",
    "                        (step, loss, accuracy))\n",
    "                step += 1\n",
    "\n",
    "            # Print validation results\n",
    "            print('validation for epoch %d' % epoch)\n",
    "            val_accuracy = self.evaluate(sess, X_val, Y_val)\n",
    "            print('-  epoch %d: validation accuracy = %.3f' % (epoch, val_accuracy))\n",
    "            \n",
    "        #############################################################################\n",
    "        # TODO: Plot training curve                                                 #\n",
    "        #############################################################################\n",
    "        # Graph 1. X: iteration (training step), Y: training loss\n",
    "\n",
    "        # Graph 2. X: iteration (training step), Y: training accuracy\n",
    "        \n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def evaluate(self, sess, X_eval, Y_eval):\n",
    "        eval_accuracy = 0.0\n",
    "        eval_iter = 0\n",
    "        for i in range(X_eval.shape[0] // self.batch_size):\n",
    "            X_ = X_eval[i * self.batch_size:(i + 1) * self.batch_size][:]\n",
    "            Y_ = Y_eval[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            \n",
    "            #############################################################################\n",
    "            # TODO: You can change feed data as you want                                #\n",
    "            #############################################################################\n",
    "            feed_dict = {self.X:X_, self.Y:Y_}\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "            accuracy = sess.run(self.accuracy_op, feed_dict=feed_dict)\n",
    "            eval_accuracy += accuracy\n",
    "            eval_iter += 1\n",
    "        return eval_accuracy / eval_iter\n",
    "    \n",
    "#    def _model(self):\n",
    "#        print('-' * 5 + '  Your model  ' + '-' * 5)\n",
    "\n",
    "        #############################################################################\n",
    "        # TODO: Implement you own model here                                        #\n",
    "        #############################################################################\n",
    "\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----  Sample model  -----\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x137532ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x137532ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x137532ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x137532ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "conv1 layer: (?, 8, 8, 48)\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x133e457f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x133e457f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x133e457f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x133e457f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "conv2 layer: (?, 4, 4, 96)\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x133061d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x133061d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x133061d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x133061d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "deep conv layer: (?, 2, 2, 256)\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x134a35f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x134a35f60>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x134a35f60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x134a35f60>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "flat layer: (?, 1024)\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x13307bc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x13307bc50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x13307bc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x13307bc50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "fc3 layer: (?, 512)\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x133825be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x133825be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x133825be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x133825be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "fc4 layer: (?, 256)\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x133d0dc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x133d0dc50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x133d0dc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x133d0dc50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "fc5 layer: (?, 10)\n",
      "-----  Start training  -----\n",
      "train for epoch 0\n",
      "iteration (0): loss = 2.671, accuracy = 0.062\n",
      "iteration (50): loss = 1.972, accuracy = 0.281\n",
      "iteration (100): loss = 1.769, accuracy = 0.359\n",
      "iteration (150): loss = 1.913, accuracy = 0.359\n",
      "iteration (200): loss = 1.526, accuracy = 0.406\n",
      "iteration (250): loss = 1.554, accuracy = 0.422\n",
      "iteration (300): loss = 1.434, accuracy = 0.422\n",
      "iteration (350): loss = 1.407, accuracy = 0.469\n",
      "iteration (400): loss = 1.618, accuracy = 0.453\n",
      "iteration (450): loss = 1.277, accuracy = 0.531\n",
      "iteration (500): loss = 1.517, accuracy = 0.469\n",
      "iteration (550): loss = 1.155, accuracy = 0.609\n",
      "iteration (600): loss = 1.256, accuracy = 0.484\n",
      "iteration (650): loss = 1.150, accuracy = 0.531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration (700): loss = 1.318, accuracy = 0.500\n",
      "iteration (750): loss = 1.646, accuracy = 0.469\n",
      "validation for epoch 0\n",
      "-  epoch 0: validation accuracy = 0.577\n",
      "train for epoch 1\n",
      "iteration (800): loss = 1.152, accuracy = 0.547\n",
      "iteration (850): loss = 1.299, accuracy = 0.547\n",
      "iteration (900): loss = 1.152, accuracy = 0.594\n",
      "iteration (950): loss = 1.183, accuracy = 0.578\n",
      "iteration (1000): loss = 1.249, accuracy = 0.547\n",
      "iteration (1050): loss = 1.057, accuracy = 0.578\n",
      "iteration (1100): loss = 1.500, accuracy = 0.484\n",
      "iteration (1150): loss = 0.986, accuracy = 0.641\n",
      "iteration (1200): loss = 1.324, accuracy = 0.500\n",
      "iteration (1250): loss = 0.999, accuracy = 0.641\n",
      "iteration (1300): loss = 0.904, accuracy = 0.656\n",
      "iteration (1350): loss = 1.102, accuracy = 0.656\n",
      "iteration (1400): loss = 0.809, accuracy = 0.703\n",
      "iteration (1450): loss = 1.317, accuracy = 0.562\n",
      "iteration (1500): loss = 1.071, accuracy = 0.578\n",
      "validation for epoch 1\n",
      "-  epoch 1: validation accuracy = 0.602\n",
      "train for epoch 2\n",
      "iteration (1550): loss = 1.181, accuracy = 0.562\n",
      "iteration (1600): loss = 1.112, accuracy = 0.672\n",
      "iteration (1650): loss = 1.067, accuracy = 0.609\n",
      "iteration (1700): loss = 1.152, accuracy = 0.594\n",
      "iteration (1750): loss = 1.003, accuracy = 0.641\n",
      "iteration (1800): loss = 0.945, accuracy = 0.672\n",
      "iteration (1850): loss = 1.028, accuracy = 0.672\n",
      "iteration (1900): loss = 0.750, accuracy = 0.797\n",
      "iteration (1950): loss = 1.101, accuracy = 0.625\n",
      "iteration (2000): loss = 0.958, accuracy = 0.672\n",
      "iteration (2050): loss = 0.898, accuracy = 0.656\n",
      "iteration (2100): loss = 0.880, accuracy = 0.703\n",
      "iteration (2150): loss = 0.964, accuracy = 0.641\n",
      "iteration (2200): loss = 1.040, accuracy = 0.641\n",
      "iteration (2250): loss = 0.903, accuracy = 0.625\n",
      "validation for epoch 2\n",
      "-  epoch 2: validation accuracy = 0.674\n",
      "train for epoch 3\n",
      "iteration (2300): loss = 0.816, accuracy = 0.703\n",
      "iteration (2350): loss = 0.778, accuracy = 0.688\n",
      "iteration (2400): loss = 0.860, accuracy = 0.766\n",
      "iteration (2450): loss = 0.877, accuracy = 0.688\n",
      "iteration (2500): loss = 0.737, accuracy = 0.781\n",
      "iteration (2550): loss = 0.704, accuracy = 0.750\n",
      "iteration (2600): loss = 0.970, accuracy = 0.641\n",
      "iteration (2650): loss = 0.740, accuracy = 0.703\n",
      "iteration (2700): loss = 0.695, accuracy = 0.719\n",
      "iteration (2750): loss = 1.172, accuracy = 0.672\n",
      "iteration (2800): loss = 0.730, accuracy = 0.703\n",
      "iteration (2850): loss = 0.847, accuracy = 0.656\n",
      "iteration (2900): loss = 0.612, accuracy = 0.797\n",
      "iteration (2950): loss = 0.981, accuracy = 0.672\n",
      "iteration (3000): loss = 0.671, accuracy = 0.797\n",
      "iteration (3050): loss = 0.841, accuracy = 0.719\n",
      "validation for epoch 3\n",
      "-  epoch 3: validation accuracy = 0.667\n",
      "train for epoch 4\n",
      "iteration (3100): loss = 0.622, accuracy = 0.766\n",
      "iteration (3150): loss = 0.652, accuracy = 0.766\n",
      "iteration (3200): loss = 0.879, accuracy = 0.703\n",
      "iteration (3250): loss = 0.670, accuracy = 0.766\n",
      "iteration (3300): loss = 0.493, accuracy = 0.859\n",
      "iteration (3350): loss = 0.491, accuracy = 0.859\n",
      "iteration (3400): loss = 0.624, accuracy = 0.750\n",
      "iteration (3450): loss = 0.520, accuracy = 0.828\n",
      "iteration (3500): loss = 0.651, accuracy = 0.750\n",
      "iteration (3550): loss = 0.753, accuracy = 0.719\n",
      "iteration (3600): loss = 0.628, accuracy = 0.797\n",
      "iteration (3650): loss = 0.730, accuracy = 0.672\n",
      "iteration (3700): loss = 0.527, accuracy = 0.828\n",
      "iteration (3750): loss = 0.750, accuracy = 0.781\n",
      "iteration (3800): loss = 0.530, accuracy = 0.766\n",
      "validation for epoch 4\n",
      "-  epoch 4: validation accuracy = 0.665\n",
      "train for epoch 5\n",
      "iteration (3850): loss = 0.540, accuracy = 0.797\n",
      "iteration (3900): loss = 0.744, accuracy = 0.750\n",
      "iteration (3950): loss = 0.612, accuracy = 0.781\n",
      "iteration (4000): loss = 0.496, accuracy = 0.797\n",
      "iteration (4050): loss = 0.695, accuracy = 0.781\n",
      "iteration (4100): loss = 0.424, accuracy = 0.797\n",
      "iteration (4150): loss = 0.550, accuracy = 0.828\n",
      "iteration (4200): loss = 0.589, accuracy = 0.734\n",
      "iteration (4250): loss = 0.635, accuracy = 0.750\n",
      "iteration (4300): loss = 0.565, accuracy = 0.828\n",
      "iteration (4350): loss = 0.801, accuracy = 0.734\n",
      "iteration (4400): loss = 0.412, accuracy = 0.891\n",
      "iteration (4450): loss = 0.194, accuracy = 0.953\n",
      "iteration (4500): loss = 0.561, accuracy = 0.781\n",
      "iteration (4550): loss = 0.470, accuracy = 0.828\n",
      "validation for epoch 5\n",
      "-  epoch 5: validation accuracy = 0.664\n",
      "train for epoch 6\n",
      "iteration (4600): loss = 0.628, accuracy = 0.844\n",
      "iteration (4650): loss = 0.503, accuracy = 0.797\n",
      "iteration (4700): loss = 0.742, accuracy = 0.719\n",
      "iteration (4750): loss = 0.555, accuracy = 0.812\n",
      "iteration (4800): loss = 0.507, accuracy = 0.828\n",
      "iteration (4850): loss = 0.465, accuracy = 0.844\n",
      "iteration (4900): loss = 0.655, accuracy = 0.781\n",
      "iteration (4950): loss = 0.622, accuracy = 0.766\n",
      "iteration (5000): loss = 0.459, accuracy = 0.844\n",
      "iteration (5050): loss = 0.499, accuracy = 0.797\n",
      "iteration (5100): loss = 0.588, accuracy = 0.781\n",
      "iteration (5150): loss = 0.391, accuracy = 0.844\n",
      "iteration (5200): loss = 0.408, accuracy = 0.875\n",
      "iteration (5250): loss = 0.316, accuracy = 0.875\n",
      "iteration (5300): loss = 0.550, accuracy = 0.812\n",
      "iteration (5350): loss = 0.354, accuracy = 0.875\n",
      "validation for epoch 6\n",
      "-  epoch 6: validation accuracy = 0.668\n",
      "train for epoch 7\n",
      "iteration (5400): loss = 0.677, accuracy = 0.766\n",
      "iteration (5450): loss = 0.481, accuracy = 0.828\n",
      "iteration (5500): loss = 0.302, accuracy = 0.891\n",
      "iteration (5550): loss = 0.521, accuracy = 0.844\n",
      "iteration (5600): loss = 0.544, accuracy = 0.781\n",
      "iteration (5650): loss = 0.461, accuracy = 0.875\n",
      "iteration (5700): loss = 0.438, accuracy = 0.844\n",
      "iteration (5750): loss = 0.391, accuracy = 0.875\n",
      "iteration (5800): loss = 0.498, accuracy = 0.828\n",
      "iteration (5850): loss = 0.417, accuracy = 0.844\n",
      "iteration (5900): loss = 0.486, accuracy = 0.828\n",
      "iteration (5950): loss = 0.408, accuracy = 0.844\n",
      "iteration (6000): loss = 0.341, accuracy = 0.906\n",
      "iteration (6050): loss = 0.237, accuracy = 0.922\n",
      "iteration (6100): loss = 0.378, accuracy = 0.891\n",
      "validation for epoch 7\n",
      "-  epoch 7: validation accuracy = 0.670\n",
      "train for epoch 8\n",
      "iteration (6150): loss = 0.621, accuracy = 0.828\n",
      "iteration (6200): loss = 0.499, accuracy = 0.812\n",
      "iteration (6250): loss = 0.338, accuracy = 0.906\n",
      "iteration (6300): loss = 0.304, accuracy = 0.875\n",
      "iteration (6350): loss = 0.422, accuracy = 0.844\n",
      "iteration (6400): loss = 0.371, accuracy = 0.828\n",
      "iteration (6450): loss = 0.368, accuracy = 0.891\n",
      "iteration (6500): loss = 0.230, accuracy = 0.891\n",
      "iteration (6550): loss = 0.264, accuracy = 0.906\n",
      "iteration (6600): loss = 0.302, accuracy = 0.875\n",
      "iteration (6650): loss = 0.351, accuracy = 0.828\n",
      "iteration (6700): loss = 0.398, accuracy = 0.828\n",
      "iteration (6750): loss = 0.394, accuracy = 0.859\n",
      "iteration (6800): loss = 0.241, accuracy = 0.922\n",
      "iteration (6850): loss = 0.216, accuracy = 0.922\n",
      "validation for epoch 8\n",
      "-  epoch 8: validation accuracy = 0.700\n",
      "train for epoch 9\n",
      "iteration (6900): loss = 0.319, accuracy = 0.859\n",
      "iteration (6950): loss = 0.286, accuracy = 0.875\n",
      "iteration (7000): loss = 0.226, accuracy = 0.938\n",
      "iteration (7050): loss = 0.151, accuracy = 0.953\n",
      "iteration (7100): loss = 0.349, accuracy = 0.844\n",
      "iteration (7150): loss = 0.196, accuracy = 0.938\n",
      "iteration (7200): loss = 0.299, accuracy = 0.906\n",
      "iteration (7250): loss = 0.297, accuracy = 0.922\n",
      "iteration (7300): loss = 0.196, accuracy = 0.922\n",
      "iteration (7350): loss = 0.306, accuracy = 0.922\n",
      "iteration (7400): loss = 0.333, accuracy = 0.906\n",
      "iteration (7450): loss = 0.390, accuracy = 0.844\n",
      "iteration (7500): loss = 0.316, accuracy = 0.891\n",
      "iteration (7550): loss = 0.247, accuracy = 0.922\n",
      "iteration (7600): loss = 0.200, accuracy = 0.938\n",
      "validation for epoch 9\n",
      "-  epoch 9: validation accuracy = 0.665\n",
      "train for epoch 10\n",
      "iteration (7650): loss = 0.169, accuracy = 0.922\n",
      "iteration (7700): loss = 0.154, accuracy = 0.922\n",
      "iteration (7750): loss = 0.264, accuracy = 0.891\n",
      "iteration (7800): loss = 0.153, accuracy = 0.969\n",
      "iteration (7850): loss = 0.139, accuracy = 0.953\n",
      "iteration (7900): loss = 0.159, accuracy = 0.938\n",
      "iteration (7950): loss = 0.266, accuracy = 0.906\n",
      "iteration (8000): loss = 0.228, accuracy = 0.891\n",
      "iteration (8050): loss = 0.227, accuracy = 0.906\n",
      "iteration (8100): loss = 0.262, accuracy = 0.938\n",
      "iteration (8150): loss = 0.176, accuracy = 0.953\n",
      "iteration (8200): loss = 0.144, accuracy = 0.969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration (8250): loss = 0.188, accuracy = 0.938\n",
      "iteration (8300): loss = 0.093, accuracy = 0.984\n",
      "iteration (8350): loss = 0.141, accuracy = 0.969\n",
      "iteration (8400): loss = 0.259, accuracy = 0.891\n",
      "validation for epoch 10\n",
      "-  epoch 10: validation accuracy = 0.680\n",
      "train for epoch 11\n",
      "iteration (8450): loss = 0.119, accuracy = 0.969\n",
      "iteration (8500): loss = 0.087, accuracy = 0.984\n",
      "iteration (8550): loss = 0.085, accuracy = 0.969\n",
      "iteration (8600): loss = 0.103, accuracy = 0.953\n",
      "iteration (8650): loss = 0.128, accuracy = 0.953\n",
      "iteration (8700): loss = 0.270, accuracy = 0.891\n",
      "iteration (8750): loss = 0.269, accuracy = 0.922\n",
      "iteration (8800): loss = 0.152, accuracy = 0.953\n",
      "iteration (8850): loss = 0.223, accuracy = 0.891\n",
      "iteration (8900): loss = 0.186, accuracy = 0.891\n",
      "iteration (8950): loss = 0.085, accuracy = 0.984\n",
      "iteration (9000): loss = 0.220, accuracy = 0.906\n",
      "iteration (9050): loss = 0.123, accuracy = 0.969\n",
      "iteration (9100): loss = 0.105, accuracy = 0.984\n",
      "iteration (9150): loss = 0.084, accuracy = 0.984\n",
      "validation for epoch 11\n",
      "-  epoch 11: validation accuracy = 0.689\n",
      "train for epoch 12\n",
      "iteration (9200): loss = 0.118, accuracy = 0.969\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-dced17d73f11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/cpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYourModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'***** test accuracy: %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-86ac567e6b16>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sess, X_train, Y_train, X_val, Y_val)\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/csci566/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/csci566/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/csci566/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/csci566/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/csci566/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/csci566/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Clear old computation graphs\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "    with tf.device('/cpu:0'):\n",
    "        model = YourModel()\n",
    "        model.train(sess, X_train, Y_train, X_val, Y_val)\n",
    "        accuracy = model.evaluate(sess, X_test, Y_test)\n",
    "        print('***** test accuracy: %.3f' % accuracy)\n",
    "        # Save your model\n",
    "        saver = tf.train.Saver()\n",
    "        model_path = saver.save(sess, \"lib/tf_models/problem2/csci-599_mine.ckpt\")\n",
    "        print(\"Model saved in %s\" % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Load your model\n",
    "model = YourModel()\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"lib/tf_models/problem2/csci-599_mine.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
